\documentclass[12pt,letterpaper,boxed]{maths_v5}
% set 1-inch margins in the documentx
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage[english]{babel}
% include this if you want to import graphics files with /includegraphics
\usepackage{graphicx}
% \usepackage{mathtools}
\usepackage{amsmath}  
\usepackage{amsthm}
\usepackage{braket} 
\usepackage{relsize} 
\usepackage{float}
\usepackage{lmodern} % math, rm, ss, tt
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage[dvipsnames]{xcolor}
\usepackage{framed}
\usepackage{tikz-cd}
\usepackage{tikz}
\usepackage[most]{tcolorbox}
\usepackage{bm}
\renewcommand{\labelenumi}{{\bf (\alph{enumi})}} 
\newcommand{\normal}{\unlhd}
%\newcommand{\ker}{\mbox{ker}}
\newcommand{\im}{\mbox{Im}}
\newcommand{\zz}{\mathbb{Z}}
\newcommand{\rr}{\mathbb{R}}
\newcommand{\cc}{\mathbb{C}}
\newcommand{\qq}{\mathbb{Q}}
\newcommand{\ff}{\mathbb{F}}

\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\renewcommand{\ker}{\mbox{Ker}}
\newcommand{\aut}{\mbox{Aut}}
\newcommand{\emb}{\mbox{Emb}}
\newcommand{\gal}{\mbox{Gal}}
\newcommand{\ch}{\mbox{Ch}}
\newcommand\mapsfrom{\mathrel{\reflectbox{\ensuremath{\mapsto}}}}

\linespread{1.1} 
\usetikzlibrary{arrows}

\theoremstyle{definition}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\newtheorem{definition}{Definition}[section]

\begin{document}
    \section*{Lecture 1, 1/22/20}
    Example: The set of all probability distributions form a manifold. 
    This is called the statistical manifold. 
    \begin{itemize}
        \item Each element in the manifold is a probability distribution; a function. 
        \item This manifold is infinite dimensional. 
        \item The set of Gaussian distributions in dimension $k$ is a submanifold. 
        \item Consider the set of all $N(\mu, \sigma)$ with $\mu \in \rr$ and $\sigma \in \rr^{+}$.
    \end{itemize}    
    Recall a probability distribution is a  function satisfying 
    \begin{itemize}
        \item[1.] $p(x) \ge 0$ 
        \item[2.] $\displaystyle \int p(x)dx = 1$ 
    \end{itemize}
    Say $N(\mu_1, \sigma_1)$ and $N(\mu_2, \sigma_2)$ are determined by 
    $\mu_1, \mu_2, \sigma_1, \sigma_2$. It's a parameterization. The correct
    way to construct a metric is to view $(\mu, \sigma) \in P_{+}$ 
    where $P_{+}$ is the Poincare upperplane. To find the distance between the points 
    $(\mu_1, \sigma_1)$ and $(\mu_2, \sigma_2)$ in the $\mu\sigma$-plane, you must find the 
    geodesic. 
    \begin{itemize}
        \item "Connections on a manifold is just a derivative" just like one 
        calls the derivative of $f: \rr^n \to \rr$ as the gradient
    \end{itemize}

    \section*{Lecture 2, 1/27/20}
    \subsection*{Abstract Manifolds and Examples.}
    \begin{example}
        When we observe a sequence of points  $\{(x^(i), y^(i))\}$ 
        in the shape of a line, we know that 
        there is some kind of correlation within the data. However, this is cheating, since we know 
        what it looks like, but the computer does not see it. A way we handle this is by making the computer 
        find the line of best fit. 
    
        The computer first locates a centroid within the data, and then rotates 
        about that vertex. But how does it find the direction to point? One way 
        to think about this is to imagine a normal vector from the line rotating 
        around a circle $S^1$, as it tries to find the best normal vector. 
    
        The problem is well defined since $S_1$ is compact; hence, a desirable optimization exists. 
        Also note that $\bm{n}$ and $-\bm{n}$ both determine the same line. Thus the set of 
        lines passing through the centroid are in one to one correspondence with 
        $S_1/\zz_2$. However, this is topologically a circle still; it's like we only focus on
        the upper half, and then identify the vertices of our semicircle together. 
    \end{example}

    \begin{example}
        Suppose we have a bunch of data points in the shape of a plane. Again, identifying 
        the fact that we approximately have a plane is easy for us, although we'd like 
        to teach a computer to understand the shape present. 

        We assign weights: 
        \[
            y = w_0x_0 + w_1x_1 + w_2x_2  
        \]
        and find $w_0, w_1, w_2$ so the plane fits the data but is not overfitting 
        nor underfitting. 

        Again, in this situation, we look for a centroid of the data, say $C$. We then parse
        through all possible planes, rotating around $C$, seeking the optimal plane. 

        \textbf{Note:} Mathematically, we want to make the configuration space 
        for the problem as simple as possible. A way to do this is to look at the normal plane 
        assigned at $C$; hence we can just look at one point to control the plane. 
        So our problem boils down to observing that points on $S^2$ correspond to all 
        of our possible planes; with the exception that $\bm{n}$ and $-\bm{n}$ correspond 
        to the same plane. Hence we are really looking at $S_2/\zz_2$. 
    \end{example}

    \begin{definition}
        A \textbf{real projective plane} $\rr P^2$ is defined to be 
        $S^2/\zz_2$. That is, a real projective plane is a set of all 
        2-planes in $\rr^3$ passing through the origin. 
    \end{definition}

    \begin{example}
        How do you count the set of lines? We presented a method before on how to do it, but there 
        is another way. We can count the lines by measuring their intersections with a vertical 
        line centered at $x = 1$. This then results in counting $\rr^1 \cup \{\infty\} \cong S_1$. 

        Another way to do this is to count by angles; $0 \le \theta \le \pi$ which 
        also corresponds to $S_1$.
        
        In either case, we obtain  the \textbf{projective real line}.
    \end{example}

    \begin{definition}
        A \textbf{differentiable manifold} of dimension $n$ is a set $M$ 
        and a family of injective mappings $x_\alpha: U_\alpha \subset \rr^n \to M$ 
        of open sets $U_\alpha$ of $\rr^n$ into $M$ such that 
        \begin{itemize}
            \item[1.] $\displaystyle \bigcup_{\alpha} x_\alpha(U_\alpha) = M$ 
            \item[2.] For any pair $\alpha, \beta$ with $x_\alpha(U_\alpha) \cap x_\beta(U_\beta) = W \ne \varnothing$ 
            the set $x_\alpha^{-1}(W)$ and $x_\beta^{-1}(W)$ are open sets in $\rr^n$ 
            and the mapping $x_\beta^{-1}\circ x_\alpha$ is differentiable. 
        \end{itemize}
    \end{definition}

    \section*{Lecture 3, 1/29/20}
    \begin{definition}
        Let $x$ be a random variable (which may be discrete, scalar or vector continuous). 
        A statistical model $M =  \{p(x, \zeta)\}$ with parameter $\zeta$ 
        is a manifold when it satisfies the desired regularity conditions. 
    \end{definition}
    Information geometry studies the invariante geometrical structures of regular statistical 
    models. 

    For the Gaussian random variable $x$, we have that 
    \[
        p(x:u, \sigma^2) 
        = \frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\dfrac{(x-\mu)^2}{2\sigma^2}\right\}.
    \]
    The set of Gaussian distributions is a two-dimensional manifold, where a point denotes a probability 
    desity function and 
    \[
        \zeta = (\mu, \sigma).
    \]
    Alternatively, we can let $m_1, m_2$ be the \textbf{first and second moments} of
    $x$, given by 
    \[
        m_1 = E[x] = \mu \qquad m_2  = E[x^2] = \mu^2 + \sigma^2.
    \]
    Then $\zeta= (m_1, m_2)$ is a coordinate system. We could even have the coordiate system 
    \[
        \zeta = (\theta_1, \theta_2) \qquad \theta = \frac{\mu}{\sigma^2}, \theta_2 = -\frac{1}{2\sigma^2}.
    \]
    called the \textbf{natural coordinates}.

    This is similar to the idea where for a smooth function $f(x)$ we have 
    \[
        f(x) = f(x_0) + (x - x_0)f'(x_0) + \frac{(x-x_0)^2}{2!}f''(x_0) + \cdots
    \]
    and with a change of coordinates
    \[
        f(x) - f(x_0) = f'(x_0)\overline{x} + \frac{f''(x_0)}{2!}\overline{x}^2 + \cdots 
    \]

    Now consider a discrete random variable with takes values on $X = \{0, 1, 2, \dots, n\}$. 
    Then if $p_i = \text{Prob}{x = i} > 0$, we can compute a probability vector 
    \[
        \bm{p} = (p_0, p_1, \dots, p_n).
    \]
    Since $\displaystyle \sum_{i = 0}^{n}p_i = 1$, one of the vectors is linearly 
    dependent, so we see  that $\bm{p}$ is $n$-dimensional. Therefore, the set of probability 
    distributions  of a discrete form the \textbf{manifold of discrete distributions}. 
    In fact, these manifolds actually 
    take the form of simplices $S_n$. The manifold becomes a $n$-dimensional simplex. 
    
    Consider the notation 
    \[
        \delta_i(x) = 
        \begin{cases}
            1 & \text{ if } x = i\\
            0 & \text{ otherwise}
        \end{cases}.
    \]
    Then we have that 
    \[
        p(x, \bm{\zeta}) = 
        \sum_{i = 1}^n\zeta_i\delta_i(x) + p_0(\bm{\zeta})\delta_0(x)
    \]
    and remember that 
    \[
        p_0 = 1 - \sum_{i =1}^{n}p_i.   
    \]
    In information geometry, we use another coordinate system $\bm{\theta}$ where 
    \[
        \theta_i = \log\left( \frac{p_i}{p_0} \right) \quad i = 1,2, \dots, n.
    \]
    That is, these are the best coordinates for $S_n$. 

    Let $x$ be a variable taking value in $N = \{1,2,\dots, n\}$. Then we assigned a positive 
    measure $m_i$ to each element $i \in N$ so that 
    \[
        \zeta= (m_1, \dots, m_n).
    \]
    This defines a distribution of meaesures over $N$. The set of all positive measures forms 
    an $n$-dimensional manifold. When we have $\sum  n = 1$, this proabbility distributiion corresponds 
    with the $S_{n-1}$ simplex. 

    An example of this occurs when we describe the brigthness of an image by discretizing 
    it into $n^2$ pixels. For each coordinate $(i, j)$, we assign a brigthness to it, 
    and this forms a positive measure. 

    Another examplee occurs with matrices. 
    The set of all $n \times n$ matrices form an $n^2$-dimensional manifold.  
    If $A$ is symmetric and positive definite, then they form a $\frac{n(n+1)}{2}$-dimensional 
    manifold,
    since we can use the elements in the upper diagonal of $A$ as a coordinate 
    system. 

    Another example includes the set of all neural networks which form a manifold. 
    A neural network is specified by weights $w_{ij}$ which connects neuron $i$ to neuron $j$. 
    The coordinate system for this manifold is the matrix $\bm{W} = (w_{ij})$. 


    \section*{Lecture 4, 2/3/19}
    Recall that  
    \begin{definition}
        A \textbf{differentiable manifold} of dimension $n$ is a set $M$ 
        and a family of injective mappings $x_\alpha: U_\alpha \subset \rr^n \to M$ 
        of open sets $U_\alpha$ of $\rr^n$ into $M$ such that 
        \begin{itemize}
            \item[1.] $\displaystyle \bigcup_{\alpha} x_\alpha(U_\alpha) = M$ 
            \item[2.] For any pair $\alpha, \beta$ with $x_\alpha(U_\alpha) \cap x_\beta(U_\beta) = W \ne \varnothing$ 
            the set $x_\alpha^{-1}(W)$ and $x_\beta^{-1}(W)$ are open sets in $\rr^n$ 
            and the mapping $x_\beta^{-1}\circ x_\alpha$ is differentiable. 

            \item[3.] The family $\{(U_\alpha, x_\alpha)\}$ is maximal relative 
            to the coordinates (1) and (2).  
        \end{itemize}
    \end{definition}

    \begin{itemize}
        \item The pair $(U_\alpha, x_\alpha)$ with $p \in x_\alpha(U_\alpha)$ 
        is called a \textbf{parameterization} (or a system of coordiantes) of $M$ at $p$. 

        \item $x_\alpha(U_\alpha)$ is called a \textbf{coordiante neigborhood}. 
        \item The family $\{(U_\alpha, x_\alpha)\}$ satisfying (1) and (2) is called 
        a \textbf{differential} structure on $M$. 
    \end{itemize}
    The reader may realize the above criteria is probably inconvenient to use to 
    check if something is a manifold. The process isn't too bad for simple, obvious shapes 
    but can be gradually more complicated. In real life, people use tricks to see if a
    set is a manfiold. 

    \begin{example}
        Consider the set $X = \{(x,y,z) = x^2 + y^2 + z^2 = 1\}$. 
        Consider the function $f: \rr^3 \to \rr$ where 
        \[
            f(x, y, z) = x^2 + y^2  + z^2.
        \]
        We can find a critical point of this by observing that 
        \[
            \nabla f =  0 \implies (2x, 2y, 2z) = (0,0,0) \implies (x,y,z) = (0,0,0).
        \]
        Therefore $f(0,0,0) = 0$ is a critical value. So $1$ is a regular value, 
        which implies that 
        $f^{-1}(1) = \{x^2 + y^2 + z^2 = 1\} = X$ is a manifold in $\rr^3$.
    \end{example}

    \begin{example}
        Consider the torus $T = S^1 \times  S^1$. Both $S^1$ are manifolds, and 
        the product of manifolds is a manifold, so that the torus is also a manifold. 
    \end{example}

    We can summarize some tricks: 
    \begin{itemize}
        \item View a given set as an image of a regular value of a differenitable map. 
        \item Form a product of a given manifold to get another manifold. 
        \item Form a \textbf{tangent bundle} $TM$ of a manifold $M$ to get another 
        manifold. The tangent bundle can often be used to understand the dynamics of the manifold.  
        \item By the method of "discontinuous action of a group."
        \item View a set as a homogeneous space. What does this mean? Let $G$  
        be a lie group
        and suppose $G$ acts on a manifold $M$ such that, for all $x, y \in M$, 
        there exists $g \in  G$ such that 
        \[
            gx = y.
        \]
        Find a subgroup 
        $H$ which fixes an element $M$. The set $G/H$ id called a 
        manifold called a \textbf{homogenous space}. 
    \end{itemize}

    \begin{example}
        Recall that $\rr P^2 = S^2/\zz_2$ where we write $\zz = \{I, A\}$. 
        Here, $I$ is the identity and $A$ is the antipodal action. For any $p \in S^2$, we 
        write 
        \[
            I(p) = p \qquad A(p) = -p
        \]
        where $-p$ can be thought of as a 180 degree rotation. We then say that $\rr P^2$ 
        is a manifold.
    \end{example}

    \begin{example}
        Let $M = S^{n-1}$, and suppose $G = SO(n)$ acts on the manifold $S^{n-1}$. 
        Let $v \in M$; then there exists a subgroup $H = SO(n-1)$ which fix $v$. For example, 
        consider the subgroup  $SO(2)$ where   
        \begin{align*}
            \begin{bmatrix}
                \cos(\theta)& \sin(\theta)& 0 \\
                -\sin(\theta)& \cos(\theta)& 0 \\
                0 & 0 & 1
            \end{bmatrix}
            \begin{bmatrix}
                0 \\
                0\\
                1
            \end{bmatrix}
            = 
            \begin{bmatrix}
                0 \\
                0\\
                1
            \end{bmatrix}
        \end{align*}
        Then we have that  $S^{n} = SO(n)/SO(n-1).$ This can also be imagined as a 
        fiber bundle. 
        \begin{center}
            \begin{tikzcd}
                SO(3) \arrow[r] & SO(3) \arrow[d]\\
                & S^2
            \end{tikzcd}
        \end{center}
    \end{example}

    \begin{definition}
        The set of all lines passing through the origin of $\rr^{n+1}$ is called 
        the \textbf{real projective space} of dimension $n$, denoted at $\rr P^n$.
    \end{definition}

    Let $(x_1, \dots, x_{n+1})\in \rr^{n+1}$. Then $\rr P^n$ can be identified as the quotient sapce $\rr^{n+1}- \{0\}$
    with the equivalence relation $(x_1, \dots, x_{n+1}) \cong (\lambda x_1, \dots, \lambda x_{n+1})$. 
    The points of $\rr P^n$ will be denoted by the equivalence class $[x_1, x_2, \dots, x_{n+1}]$. 
    Notice that 
    \[
      [x_1, \dots, x_{n+1}] = [\frac{x_1}{x_i}, \dots, 1, \dots, \frac{x_{n+1}}{x_i}].
    \]
    Define the subsets $V_1, V_2, \dots, V_{n+1}$ of $\rr P^n$ by 
    \[
        V_i = \{[x_1, x_2, \dots, x_{n+1}] \mid x_i \ne 0 \} \quad i = 1,2, \dots, n.
    \]
    The claim to prove now is that $\rr P^n$ can be covered by the sets $V_1, \dots, V_n$. 
    This is obtained by the maps 
    \[
        x_i(y_1,  \dots, y_n) = [y_1, \dots, y_{i-1}, 1, y_{i+1}, \dots, y_n] 
    \]
    where $y_{j} = \frac{x_j}{x_i}$. 

    One then has to check that $V_i \cap V_j \ne 0$ and that $x^{-1}(V_i \cap V_j)$ is open.
    One then must check that  
    \begin{align*}
        x_j^{-1}\circ x_i(y_1, \dots, y_n)= x_j^{-1}([y_1, \dots, y_{i-1}, 1, y_{i+1},  \dots, y_n])\\
        = x_j^{-1}(\frac{y_1}{y_j}, \dots, \frac{y_{j-1}}{y_j}, 1, \frac{y_{j+1}}{y_j}, \dots, \frac{y_{i-1}}{y_j}, \frac{1}{y_j}, \frac{y_{i+1}}{y_j}, \dots, \frac{y_n}{y_j} )\\
        = \frac{y_1}{y_j}, \dots, \frac{y_{j-1}}{y_j}, 1, \frac{y_{j+1}}{y_j}, \dots, \frac{y_{i-1}}{y_j}, \frac{1}{y_j}, \frac{y_{i+1}}{y_j}, \dots, \frac{y_n}{y_j}.
    \end{align*}

    \section*{Lecture 5, 2/5/19}
    Last time we discussed the real projective space $\rr P^n$. Today, we'll discuss 
    the space $\cc P^{n}$, a manifold of real dimension $2n$, which is the set of complex lines in $\cc^{n+1}$. This 
    is known as the complex projective space, and is constructed similarly to how the real 
    projective plane is constructed. 

    \begin{definition}
        Let $M_1$ and $M_2$ be $n$ and $m$-dimensional manifolds, respectively. 
        Then a mapping 
        \[
            \phi: M_1 \to M_2
        \]
        is differentiable at a point $p \in M_1$ 
        if, given a parameterization
        \[
            \overline{y}: V \subset \rr^m \to M_2 
        \]
        containing $\phi(p)$, there exists a parameterization 
        \[
            \overline{x}: U \subset \rr^n \to M_1     
        \]
        with $p \in U$ such that $\phi(\overline{x}(U)) \subset \overline{y}(V)$
        and the mapping 
        \begin{align*}
            \overline{y}^{-1} \circ \phi \circ \overline{x}: U \subset \rr^n \to \rr^m
        \end{align*}
        is differentiable at $\overline{x}^{-1}(p)$. It suffices to show that all partial derivatives 
        of $\overline{y}^{-1}\circ \phi \circ \overline{x}$ exist and are continuous.
    \end{definition}
    Note that because of our definition of a manifold, the above definition does  
    not depend of a choice of parameterization. 

    The map $\overline{y}^{-1}\circ \phi \circ \overline{x}$ is called the expression of $\phi$ 
    in the parameterization  $\overline{x}$ and $\overline{y}$ 

    There aren't many cases for how $n$ and $m$ can behave. 
    \begin{description}
        \item[$\bm{n = 1, m > 1}$.]
        Let $M$ be a differentiablemanifold. A differentiable curve 
        $\alpha:(a,b) \subset \rr \to M$
        is  a differentiable curve in $M$  

        \item[$\bm{n >1, m = 1}$.] 
        This is when we smash a manifold into a real line. This is a strategy 
        in Morse Theory. 
        
        \item[$\bm{n = 1, m = 1}$.]  This case is pretty  clear. 
    \end{description}

    How do we define an abstract tangent vector? We define an abstract tangent 
    vector as a differentiable operator. This is because we donn't have an ambient 
    space for $M$ to live.  

    Recall: In $\rr^n$, let $\alpha: (-\epsilon, \epsilon) \to \rr^n$ be a differenitable 
    curve with $\alpha(0) = p$. Write $\alpha(t)$ as 
    \[
        \alpha(t) = (x_1(t), x_2(t), \dots, x_n(t))
    \]
    with $t \in (-\epsilon, \epsilon)$. Let $\bm{v} = \alpha'(0) = 
    (x_1'(0), x_2'(0), \dots, x_n'(0))$. Let $f$ be a differentiable function defined 
    in a neighborhood of $p$. We can restrict the curve $
    alpha$ and express the directional derivative with respect to the vector $\bm{v} \in \rr^n$ as 
    \begin{align*}
        \frac{df\circ\alpha}{dt}\big|_{t = 0} = \frac{d}{dt}f(x_1(t), x_2(t), \dots, x_n(t))\big|_{t = 0}\\
        = \sum_{i=1}^{n}\frac{\partial f}{\partial x_i}\big|_{t=0}\frac{dx_i}{dt}\big|_{t=0}\\
        = \left( \sum x_i'(0) \frac{\partial}{\partial x_i} \right)f.
    \end{align*}
    The characteristic property of this is that the directional derivative with respect to $\bm{v}$ 
    is an operator on  differenitable functions that depend uniquely on $\bm{v}$. 

    Note that what we're doing here is pairing vectors $\alpha'(0)$ with covectors $f:\rr^n \to \rr$; 
    or in other words, simply differentiable functions which return a value given a desired direction. 

    Gu says to imagine the vectors $\bm{v}$ as moms, and the babies as all the curves $\alpha$ for which 
    $\alpha'(0)= \bm{v}$. 

    \begin{definition}
        Let  $M$ be a differentiable manifold. Let $\alpha:(-\epsilon, \epsilon) \to M$ be 
        a differentiable curve in $M$ with $\alpha(0) = p \in M$. The \textbf{tangent vector}
        to the  curve at  $t = 0$ is a function $\alpha'(0): \mathcal{D} \to \rr$ 
        where $\mathcal{D}$ is the set of differenitable functions at $p$ which take in $\alpha'(0)$. That is, 
        \[
            \alpha'(0)(f) = \frac{df\circ \alpha(t)}{dt}\big|_{t=0}
        \]
    \end{definition}

    Now a tangent vector at $p$ of $M$ is the tangent  vector at $t = 0$ of some curve  
    $\alpha:(-\epsilon, \epsilon) \to M$ with $\alpha(0) = p$

    \begin{definition}
        We say that the \textbf{tangent space} at of $M$ at $p$ is the set 
        $T_pM$, the set of all tangent vectors to $M$.
    \end{definition}

    \textbf{Claim:} $T_pM$ is a vector space. Moreover, if we choose a parameterization 
    $\overline{x}: U \to M$ then $T_pM$ has a basis $\{\frac{\partial}{\partial x_1}, \frac{\partial}{\partial x_1}, \dots, \frac{\partial}{\partial x_n})\}$. 

    Now we define a \textbf{dual basis} of $\{\frac{\partial}{\partial x_1}, \frac{\partial}{\partial x_1}, \dots, \frac{\partial}{\partial x_n})\}$
    to be 
    \[
        \{ dx_1, dx_2, \dots, dx_n\}.
    \]
    Those are differenntial "general" 1-forms of "base." For example: $3dx_1 + 5x_2^2dx_2 + 20x_1dx_3$. 

    A key idea  is  to mimick the basis  onn $\rr^n$. Recall taht $\{v_1, v_2, \dots, v_n\}$ is orthonormal 
    if $v_i \cdot v_j = \delta_{ij}$. Say $w = a_1v_2 + \cdots + a_nv_n$. 
    Then to obtain $a_i$, we simply dot the expression by $v_i$. This is the purpose of 
    the dual basis, since we  want to mimick this kind of behavior. Hence we have that 
    for a basis $\{e_1, e_2, \dots, e_n\}$, we define the dual basis $\{e_1^*, e_2^*, \dots, e_n^*\}$ 
    where 
    \[
        e_i^{*}(e_j)=  \delta_{ij}.
    \]
    For example, in linear regression, we have 
    \[
        y= \theta_0x_0 + \theta_1x_1 + \cdots + \theta_nx_n = 
        \begin{pmatrix}
            \theta_0 & \theta_1 & \cdots & \theta_n
        \end{pmatrix}
        \begin{pmatrix}
            x_0\\
            x_1\\
            \vdots\\
            x_n
        \end{pmatrix}
    \]

    Hence we have that $dx_i\left( \frac{\partial}{\partial x_j} \right) = \delta_{ij}$

    \section*{Lecture  6, 2/10/19}
    Recall that on $\rr^n$, every symmetric, positive definite matrix gives rise 
    to an inner product on $\rr^n$. We define it as 
    \[
        \left< x, y \right>_A = x^{T}Ay.
    \]
    Recall that an inner product is positive definite, symmetric, and bilinear. And note that 
    \begin{itemize}
        \item[1.]  $\left< x,x \right> = x^TAx \ge 0$
        \item[2.]  $\left<x, y \right> = x^TAy= (x^Tay)^T = y^TA^T(x^T)^T = y^TAx = \left<y, x  \right>$. 
        \item[3.] $\left<x, ay +bz  \right> = a\left<x,  y\right> = b\left<x, z\right>$.
    \end{itemize}
    Hence we really do have an inner product. 

    \textbf{Conjugate Gradient Method} on a Euclidean space is an algorithm for the numerical solution 
    of a  particular systems of linear equations, namely those who matrix is symmetric and positive definite. 
    That is, we care about 
    \[
        Ax = b
    \]
    where $A$ is symmetric, positive definite. But Prof Gu: does this occur often, i.e,
    do we often run into these types of problems? Yes. Examples include the least 
    squares or maximum likelihood extimation. And these types of problems arise in 
    supervise learning. Assume a model: 
    \[
        y = \theta_0 + \theta_1x_1 + \cdots + \theta_nx_n.  
    \]
    The data to be fit is of the form $\{(x^{(i)}, y^(i))\}_{i = 1}^{N}$. Then 
    we desire the best $\theta_i$'s where 
    \[
        y^{(i)} = \theta_0 + \theta_1x_1^{(i)} + \cdots + \theta_nx_n^{(i)}. 
    \]
    Hence we have the matrix equation 
    \[
        \begin{pmatrix}
            \vdots \\
            y^{(i)}\\
            \vdots
        \end{pmatrix}
        = 
        \begin{pmatrix}
            \vdots & \vdots & \vdots & \vdots\\
            1 & x_1^{(i)} & x_2^{(i)}& \cdots & x_n^{(i)}\\
            \vdots & \vdots & \vdots & \vdots
        \end{pmatrix}
        \begin{pmatrix}
            \theta_0\\
            \theta_1\\
            \vdots\\
            \theta_n
        \end{pmatrix}
    \]
    The conjugate gradient method is often applied to sparse systems that are too large to be handled by a direct 
    implementation or other direct methods such as the Cholesky  decomposition. It can 
    also be used to solve unconstrained optimization problems such as energy minimization.  

    Thus we care about the equation $Ax= b$ where $A$ is symmetric. Note however that if $A$ 
    is not even symmetric, we can multiply both sides by $A^T$ to observe that 
    \[
        Ax= b \implies A^TAx = A^Tb
    \]
    and then apply our method since $A^TA$ is positive definite and symmetric. 

    \begin{definition}
        Let $u, v$ be vectors. Then we say $u,v$ are conjugate if 
        \[
            u^TAv= 0.
        \]
        Since $A$ is symmetric and positive definite, the left hand side defines an inner product:
        \[
            \left<u,v  \right>_A := \left<Au,  v \right> = \left<u, A^Tv \right> = \left<u, Av \right> = u^TAv.
        \]
        (from wikipedia).
    \end{definition}

    Note that a conjugate pair of vectors are orthogonal in the sense of the 
    inner product induced by a  symmetric positive definite matrix. 
    Now consider a set $P = \{p_1, p_2, \dots, p_n\}$ of $n$ mutually conjugate, 
    vectors, with respect to some 
    symmetric postiive definite matrix $A$.
    In this inner product, we can turn this set into an orthonormal one.
    Then this set forms an orthonormal basis, and the solution can be expressed 
    in this basis. 

    Hence let $x_* = \sum_{i = 1}^{n}\alpha_ip_i$. Note that this is extremely 
    useful. For example, if we want the $\alpha_i$ coefficient, then 
    we can take the inner product with respect to $A$ to extract it. That is, 
    \[
        p_i^TAp_j = \delta_{ij}.
    \]
    Also observe that we have
    \begin{align*}
        Ax_* &= \sum_{i = 1}^{n}\alpha_iAp_i\\
        p_k^TAx_* &= \sum_{i = 1}^{n}p_k^T\alpha_iAp_i\\
        p_k^Tb &= \sum_{i = 1}^{n}\alpha_i\delta_{ik}
        \implies \left<p_k, b\right> = \alpha_k\left<p_k, p_k  \right>\\
    \end{align*}
    so that $\alpha_k = \dfrac{\left< p_k, b \right>}{\left<p_k, l_k \right>_A}$.
    Thus orthonormal bases give us a systematic way of finding the $\alpha_i$ coefficients. 
    Note that we don't need to find all elements of $P$; we can still approximate the 
    system. Thus this is an interative method. This is the same idea where in PCA, we throw away the small eigenvalues and their directions 
    since we only care about the larger ones. 

    Note that since $Ax_* = b$, we see that $x_*$ is the unique minimization of 
    \[
        f(x) = \frac{1}{2}x^TAx = - x^Tb. 
    \]
    The existence of a minimal vector is guaranteed by the fact that 
    \[
        Df(x) = \frac{1}{2}(Ax + x^TA) - b = Ax - b.
    \]
    so 
    \[
        D^2f(x) = A. 
    \]
    Hence the critical point is the global minimum! Thus to begin the algorithm, we 
    set $p_0 = b - Ax_0$, and calculate onwards by taking $p_i$ to be orthogonal (in our 
    inner product) to be orthogonal to $p_{i-1}$. Each time, we check the \textbf{residual}
    \[
        r_k = b - Ax_k
    \]
    and calculate onwards, stopping when we are fine with the error.

    \begin{definition}
        A \textbf{real Grassmanian manifold} $G_k\rr^n$ is the set of $k$-planes 
        through the origin $\rr^n$.  
        On the other hand, a \textbf{Stiefel manifold} $V_{k}\rr^n$ consists of $k$-orthonormal frames in $\rr^n$. 
        Hence one can be viewed as a submanifold of the other. 
    \end{definition}
    Note: in general, $G_k\rr^n \cong G_{n-k}\rr^n$. 

\section*{Lecture 7, 2/12/19: Riemannian manifold and metric.}
Recall that a vector space of dimension $n$, denoted 
$V^n$, equipped with an inner product $\left< \right>$, forms an \textbf{inner  product  space}. 
We  call it a Euclidean space is $n < +\infty$. Also recall that for a 
regular surface, equipped with a first fundamental form, forms a Riemannian 
surface. 

Suppose we have a manifold $M$. Let $p$ be a point, and consider the tangent 
plane $T_pM$ equipped with a metric. . Then if we vary our point within some neighborhood, we obtain 
another similar tangent plane equipped with a different metric. We want these metrics to 
continuously coincide with one another as we drag the points closer; hence we obtain some 
notion of a differentiable operator. 

We can generalize this concept even further to define a differentiable manifold equipped with 
a Riemannian metric forms a \textbf{Riemannian manifold.}


\begin{definition}
    A \textbf{Riemannian  metric} on a differentiable $n$-manifold $M$ is 
    a correspondence which associates to each $p\in M$ an inner product $\left<  \right>_p$ 
    on the tangent space $T_pM$ which varies differentiably 
    in the following sense. 

    If $x: U \subset \rr^n \to M$ is a system of coordinates around $p$, 
    with $x(x_1, x_2, \dots, x_n) = q \in X(U) \subset M$ 
    and $\dfrac{\partial}{\partial x_i}(q) = dx_q(0, \dots, 1, \dots, 0)$, 
    then 
    \[
        \left< \frac{\partial}{\partial x_i}(q),     \frac{\partial}{\partial x_j}(q) \right>_q
        =
        g_{ij}(x_1, \dots, x_n)
    \] 
    is a differentiable function on $U$. The function $g_{ij}$ is called the \textbf{local 
    representation of  the Riemannian metric}. 
\end{definition}

\begin{definition}
    Let $M$ and $N$ be Riemannian manifolds. A diffeomorphism $f:  M \to N$
    is called a \textbf{isometry} if  
    \[
        \left< u, v  \right>_p =\left<df_p(u), df_p(v) \right>_{f(p)}  
    \]
    for  all $p \in M$ and $u, v \in T_pM$. Under these conditions, we then 
    say that $M$ and $N$ are isometric. 
\end{definition}

On the other hand, we can define local isometries. 

\begin{definition}
    Let $M$ and $N$ be Riemannian manifolds. A differentiable mapping $f: M \to N$ 
    is a \textbf{local isometry}  at  $p \in M$ if there 
    exists a neigborhood $U \subset M$  of $p$ such that $f: U \to f(U)$ 
    is a diffeomorphism and $df_p$ preserves the inner product, i.e., 
    \[
        \left< u, v  \right>_p =\left<df_p(u), df_p(v) \right>_{f(p)}  
    \]
    for  all $p \in M$ and $u, v \in T_pM$. 
\end{definition}

\subsection*{Lie Groups.}
\begin{definition}
    A \textbf{lie group} $(G, \cdot)$ is a group which is also a manifold 
    with a differenitable structure such that the group operations, 
    multiplication $\cdot$ and inverse $\phantom{m}^{-1}$, are differentiable. 
    That is, the map 
    \begin{align*}
        &G \times G \to G\\
        &(x, y) \mapsto xy^{-1}
    \end{align*}
    is differentiable. That is, the group stucture and manifold structureare both 
    compatible.
\end{definition}

Examples of Lie groups include $S^1, S^3, SO(3), O(n), U(n), \dots$. 
We can make even more examples by multiplying the Lie groups together, 
since the product of lie groups forms a lie group. For example, the product  
$SO(3) \times SO(3)\times SO(2)$ is a Lie group, which models a robotic arm.

A Lie group is equipped with both left and right transformations.  For example, 
the \textbf{left transformation} is a map where 
\begin{align*}
    &L_x: G \to  G\\
    &y \mapsto xy = L_x(y)
\end{align*}
and a \textbf{right transformation} is similarly a map where 
\begin{align*}
    &R_x: G \to G\\
    &y \mapsto yx = R_x(y).
\end{align*}

With that said, we may define a Riemannian metric which is \textbf{left invariant} 
to be one for which 
\[
    \left<u, v \right>_y = \left< (dL_x)_y(u),(dL_x)_y(v)  \right>_{L_x(y)}
\]
for all $x, y \in G$ and $u, v \in T_yG$. The definition is similar for a 
\textbf{right invariant} Riemannian metric. If a Riemannian metric is both left and right invariant, 
we say it is \textbf{bi-invariant}.

\begin{thm}
    If a Lie group $G$ is compact, then there exists a bi-invariant 
    metric on $G$. 
\end{thm}

\subsection*{Product Metric.}

Let $M_1, M_2$ be Riemannian manifolds. Consider the manifold $M_1\times M_2$. 
Let $\pi_1: M_1\times M_2 \to M_1$, and $\pi_2: M_1 \times M_2 \to M_2$ be the 
natural projection maps. Introduce the Riemannian metric on $M_1\times M_2$, 
where for $u, v \in T_{(p,q)}M_1 \times M_2$, 
\begin{align*}
    \left<u , v  \right>_{(p, q)}  = \left<d\pi_1(u), d\pi_2(v)  \right>_p
    + \left<d\pi_2(u), d\pi_2(v)\right>_q
\end{align*}
for all $(p, q)\in M_1\times M_2$. 

For example, consider the $n$-torus 
\[
    T^n = S^1\times S^1 \times \cdots\times S^1
\]
where the product repeats $n$ times.
We can put a Riemannian metric on each $S^1 \subset \rr^2$, and  
extend this to a Riemannian metric on $T$. 
Then the torus $T^n$ with this metric is called a \textbf{flat torus}.

\begin{thm}
    Every differentiable manifold has a Riemannian metric. 
\end{thm}

Now with a Riemannian metric on a differentiable manifold, 
we can define all kinds of measurements, including the length  
of a curve on a manifold: 
\[
    \int_a^{b} = \sqrt{\left<\frac{d\alpha}{dt} , \frac{d\alpha}{dt} \right>}dt.
\] 
We can also find the volume. For any point $p$, obseve that 
\[
    x_i(p) = \frac{\partial}{\partial x_i}(p) = a_{i1}e_1 + \cdots + a_{in}e_n
\]
and 
\[
    x_k(p) = \frac{\partial}{\partial x_k}(p) = a_{k1}e_1 + \cdots + a_{kn}e_n.
\]
We can then define 
\[
    g_{ik} = \left< x_i(p), x_k(p) \right> 
\]
which builds a matrix $G = [g_{ik}] = A^TA$. We then have that  
\[
    \text{vol}(x_1(p), \dots, x_n(p)) = \det(A) = \sqrt{\det(G)}
\]

\section*{Lecture 8, 2/17/20: Grassman Manifolds}
Recall that the Riemannian metric on a manifold $M$ is basically 
the assignment of an inner product on each tangent space of $M$ which changes 
smoothly. Also recall that a Lie group is a manifold with a group structure. 
Examples include 
\begin{align*}
    O(n) &= \{A \in M_{n\times n}(\rr) \mid A^TA = 1\}\\
    SO(n) &= \{A \in M_{n\times n}(\rr) \mid \det A = 1, A^TA = I\} 
\end{align*}
Note that for $A(t), B(t)\in O(n)$, we have that 
\[
    [A(t)B(t)]' = A'(t)B(t) + A(t)B'(t).
\]
We can also define the tangent vector for a curve $A(t) \in O(n)$
where $A'(0) = I$. Observe that $A^TA = I$ implies that 
\begin{align*}
    [A^TA]' = T' \implies [A^T(t)]'A(t) + A^T(tA'(t) = 0
    \implies [A'(t)]TA(t) + A^T(t)TA(t)\\
    \implies [A'(0)]^T = -A'(0)
\end{align*}
after plugging in $t = 0$. This then implies that $A'(0)$ is a skew symmetric 
matrix. The nice thing about skew symmetric matrices is that the exponential matrix 
becomes an orthogonal matrix. Thus we see that the tangent space of $O(n)$ consist of 
skew symmetric $n\times n$ matrices. 

We also sometimes deal with the concept of a Lie subgroup. For example, 
$SO(2)$ is a Lie subgroup of $SO(3)$. More generally, we also have that 
$SO(n)$ is a Lie subgroup of $O(n)$. 

\begin{definition}
    A \textbf{homogeneous space} is a differentiable manifold $M$ of the form $G/H$, where 
    $G$ is a Lie group and $H$ is a \textbf{isotropy subgroup}.
\end{definition}
Examples include $O(n), SO(n), G_k\rr^n$, and $V_k\rr^n$. 

\begin{definition}
    Let $G$ be a group. Then $G$ acts (left) on a set $X$ if there is a function 
    $\phi: G\times X \to X$ where $(g,x) \mapsto \phi(g, x)$ which satisfying the following 
    properties. 
    \begin{itemize}
        \item[1.] For all $x$, $\phi(e, x) = x$. 
        \item[2.] For all $g, h \in G$, and for each $x \in X$, we have that 
        \[
            \phi(gh, x) = \phi(g, \phi(h,x)).
        \]
    \end{itemize}
    Acting \textbf{right} on a set is defined similarly. 
\end{definition}

Finally, we define the isotropy subgroup. 

\begin{definition}
    Suppose $G$ acts on a manifold $M$. Then the stabilizer of the group action, $Gx$, 
    is the \textbf{isotropy subgroup}.
\end{definition}

Recall that a \textbf{Grassmanian manifold} is the set of all $k$-planes which 
pass through the origin of $\rr^n$. 

For example, consider $G = SO(3)$ and $M = G_2\rr^3$. Take an element in $G_2\rr^3$; 
say $xy$-plane. The isotropy subgroup of $SO(3)$ is all the group elements of $SO(3)$ 
whihch fixes the $xy$-plane in $G_2\rr^3$. However, this isotropy subgroup is then 
then 
\[
    \left\{ 
    \begin{pmatrix}
    \cos(\theta) & -\sin(\theta) & 0 \\
    \sin(\theta) & \cos(\theta) & 0\\
    0 & 0 & 1
    \end{pmatrix}, \theta \in [0, 2\pi) \right\}
    \cong SO(2).
\]
Therefore, we can write this as 
\[
    G_2\rr^3 \cong SO(3)/(SO(2)\times SO(1)).
\]
where $SO(1)$ appears since we not only fix the $xy$-plane, but we also fix the 
$z$-axis. 

As another example, consider $G_2\rr^4$. Then we can imagine these 2-planes passing through the 
origin of $\rr^4$ as a rotation which fixes one 2-plane, and its orthogonal complement, 
another 2-plane. Then we have that 
\[
    G_2\rr^4 \cong SO(4)/(SO(2)\times SO(2)).
\]

Suppose $H_x$ is the isotropy group of $x$ and $H_y$ is the isotropy group of $y$, with $x,y \in M$.
Then if the group action is transitive, $H_x \cong H_y$. This is because $y = gx$ for some $g\in G$. Thus 
$H_y = gH_xg^{-1}$; so they are not only isomorphic, but they are also isotropic. 

Now in general, we have that 
\[
    G_k\rr^n \cong SO(n)/(SO(k)\times SO(n-k)).
\]

Grassmanian manifolds are particularly useful since we can look at the tangent planes of 
an $n$-manifold, and instead move their $n$-planes to the origin of $\rr^n$. For example, 
we can embed the tangent 2-planes by passing them through the origin of $\rr^3$. This 
results in obtaining a curve in $G_k\rr^n$. 

Also recall that a \textbf{Stiefel manifold}  is the set of all $k$ orthonormal frames 
in $\rr^n$. That is, we can imagine them as matrices with orthonormal columns.

\section*{Lecture 9, 2/19/20}

Today we'll consider how to take derivatives on a manifold. On a manifold, derivatives 
are called \textbf{connections}. Recall in calculus, we studied vector fields $f: \rr^n \to \rr^m$. 
First, we considered \textbf{affine connections}.

Recall that $f: \rr^n \to \rr$. If $n = 2$, we can turn this into a
graph $(x, y, f(x, y))$. For some point $p = (p_1, p_2)$ in the $xy$-plane, we can examine the 
derivative on the manifold at $(p_1, p_2, f(p_1, p_2))$. Now consider a staight line in the $xy$-plane 
given by $\alpha(t)= p + tv$ which passes through $p$. Then this line forms a curve on our manifold. 
The curve is given by $\beta(t) = f(\alpha(t))$. Now observe that 
\[
    \beta'(t) = f'(\alpha(t))\alpha'(t) = f'(p + tv)v. 
\]
which is the \textbf{directional derivative}. We can examine this further:
observe that if $v = (v_1, v_2)$, then
$f(\alpha(t)) = f((p_1, p_2) + t(v_1, v_2)) = (p_1 + tv_1, p_2 + tv_2)$. 
We can then say $x(t) = p_1 + tv_1, y(t)=  p_2 + tv_2$. Then 
\[
    \beta'(t) = f(x(t), y(t)) = \frac{\partial f}{\partial x}x'(t) + \frac{\partial f}{\partial y}y'(t)
    \implies 
    \beta'(p) = 
    \begin{pmatrix}
        \frac{\partial f}{\partial x}x'(t)\\
        \frac{\partial f}{\partial y}y'(t)
    \end{pmatrix}
    \cdot 
    \begin{pmatrix}
        x'(0)\\
        y'(0)
    \end{pmatrix}.
\]
We then have the directional derivative $\nabla f \cdot v$. More generally, 
we can consider a function $f: M^n \to \rr$ and define the directional derivative as 
\[
    D_vf = \left< \nabla f, v \right>_g
\]
where the inner product is the Riemannian metric. Note we can think of this more 
abstractly as a vector, $v$, eating a function $f: M^n \to \rr$. 
Recall that 
\begin{itemize}
    \item $a(v_p + bw_p)[f] =  av_p[f] + bw_p[f]$
    \item $v_p[af + bg] = av_p[f] + bv_p[g]$
    \item $v_p[fg] = v_p[f]\cdot g(p) +f(p)\cdot v_p[g]$
\end{itemize}
If we can take the derivative of a vector $rr^n$, we can take the derivatives of vector 
fields on our manifold. 

\begin{definition}
    Let $W$ be a vector field in $\rr^3$ and let $v$ be tangent vector on a 
    manifold $M$ in $\rr^3$. Then the \textbf{covariant derivative of} $W$ 
    with respect to $v$ is the tangent vector 
    \[
        \nabla_vW= W(p + tv)'(0).
    \]
    at the point $p$. Here, we view $W$ as a function we are differentiating, and 
    $v$ as what we're taking the derivative with respect to (i.e., the direction).
    Moreover, $\nabla_v W$ measures the initial rate of change of $W(p)$ as $p$ moves in 
    the direction of $v$. 
\end{definition}

For example, consider the vector field $F: \rr^n \to \rr^m$ where 
\[
    F(x_1, \dots, x_n) = [f_1(x_1, \dots, x_n), f_2(x_1, \dots, x_n), \dots, f_m(x_1, \dots, x_n)] 
\]
where $f_i: \rr^n \to \rr$ are real valued functions. Then 
\[
    \nabla_vF = (\nabla f_1\cdot v, \nabla f_2\cdot v, \dots, \nabla f_m\cdot v).
\]
With that said, we still have the following properties, for all 
vector fields $Y,Z$, numbers $a,b$, and tangent vectors $v$ and $w$ of $p$.
\begin{itemize}
    \item[1.] $\nabla_{av + bw}Y = a\nabla_v Y + b\nabla_W Y$
    \item[2.] $\nabla_v(aY  + bZ) = a\nabla_vY + v\nabla_v A$
    \item[3.] $\nabla_v(fY) = v[f]Y(p) + f(p)\nabla_vY$ with $f$ a differentiable function and $v[f]$ the directional derivative
    \item[4.] $v[Y\cdot Z] = \nabla_vY\cdot Z(p) + Y(p)\cdot \nabla_v Z$
\end{itemize}
In general, there's no reason to memorize these rules; all derivatives behave in the same way! 
They always satisfy linearity, distributivity, and the product rule.

To study the intrinsic geometry of a surface, one needs to generalize Gauss' idea regarding normal vectors 
mapped to the unit sphere. 
\[
    dN_p: T_p(S) \to T_p(S).   
\]
The Gauss map basically shoves all of the tangent planes 
into $G_1\rr^3$, where we normalize the normals. Specifically, 
we take a local parameterization $x(u, v)$ on a manifold for a point of interest. 
We then take the derivative $x_u, x_v$ and state 
\[
    N = \frac{x_u \times x_v}{||x_u\times x_v||} 
\] 

\begin{definition}
    Let $W$ be a vector field, and consider a point $p$ on our manifold $M$. Let $y$ 
    be a tangent vector at $y$. If $U$ is a neighborhood  at $p$, we consider a curve 
    $\alpha: (-\epsilon, \epsilon) \to U$ where $\alpha(0) = p$ and $\alpha'(0) = y$. 
    Let  $w(t): (-\epsilon, \epsilon) \to U$, $W(\alpha(t))$, be the restriction of $W$ on the curve $\alpha$. 
    Then the \textbf{covariant derivative at} $p$  \textbf{of the vector field } $W$ 
    relative to $y$ is  the normal projection of $w'(0)$ onto the plane $T_p(S)$. 
    
    This quantity  is denoted as  $\dfrac{Dw}{dt}(0)$. 
\end{definition}

\section*{Lecture 10, 2/24/20}
Recall the concept of the covariant derivative. We imagine a vector field defined 
on our manifold; this means that at every point on our manifold, there 
is a vector. We then project these vectors down onto the tangent planes from which they 
originate to extract a type of derivative. 

On the other hand, an affine connection on a differentiable manifold is a 
function $\nabla(X,Y)$ which takes vector fields $X,Y$ and outputs $\nabla_XY$. 

What's interesting to consider when this vector field is normal to the tangent plane; 
hence the projection on the tangent plane becomes zero. 

\begin{definition}
    A vector field $w$ along a paramterized curive $\alpha: I \to M$ on our manifold 
    $M$ such that 
    $Dw(\alpha)/dt = 0$ for every  $t \in I$ is called \textbf{parallel}.  
\end{definition}

\begin{definition}
    Let $M$ be a differentiable manifold, and suppose it has an affine connection 
    and a Riemannian metric $\left< \right>$. Then a connection is \textbf{compatible}
    with the metri when for any parallel vector fields $P, P'$ on $c$ we have that  
    $\left< P,P' \right>$ is constant.
\end{definition}

The idea of a covariant derivative is analgous to the idea of a dot product. Based on its properties, 
we can come up with all kinds of different ways of defining metrics. With the covariant derivative, we have a  
similar case since we can come up with all kinds of different derivatives by interchanging  $X$ and $Y$. 

\section*{Lecture 12, 3/2/20}
Currently, the largest open problems in machine learning involve optimization. 
The current most effective optimization methods include Newton's Method and the 
Conjugate Gradient method. Machine learning also requires problems to be well-modeled, 
which is currently achieved with Stiefel and Grassmanian manifolds. 

Recall that Newton's method simply seeks to find the minimum values of a single 
variable function $f(x)$ over $\rr$. This is achieved by inputing an initial guess,
which leads to a sequence $\{x_k\}$  that converges to a minimum input value. 

Let $Z$ be any $n\times p $ matrix. We want to decompose $Z$ into tangential 
and normal components. 
That is, we seek a form 
\[
    Z = \pi_n(Z) + \pi_p(Z).    
\]
\textbf{Claim:}  $\pi_n(Z) =Y\text{sym}(Y^TZ)$ and 
$\pi_T(Z)  = Y\text{skew}(Y^TZ) + (I-YY^T)Z$. 
First,  recall that 
\[
    A = \text{sym}(A) + \text{skew}(A) = \frac{A + A^T}{2} + \frac{A - A^T}{2}. 
\]
Also recall that 
\[
    g_E(\Delta_1, \Delta_2) = \text{tr}\Delta_1^T\Delta.
\]
The normal space at a point $y$ consists of all matrices $N$ 
which satisfy $\{N \mid \text{tr}(\Delta^TN) = 0 \text{ for all } \Delta \in T_pM\}$.
The set of $\{ (Y, N) \mid Y \in M, N \in \text{ normal space at } Y \}$
is called the tangent bundle of $M$. 

Note $\pi_N(Z) = Y\text{sym}(Y^TZ)$ is perpendicular to $T_YV_{P, N}$.
Since $\text{tr}(\Delta^T\Pi_N(Z)) = \text{tr}(\Delta^TY\frac{Y^TZ + Z^TY}{2})$, 
note that $\Delta^TY$ that this is a skew symmetric matrix. However, we already 
know that $\text{sym}(Y^TZ)$ is symmetric. But the trace of a skew symmetric matrix multiplied a 
symmetric matrix is zero. Hence we see that 
\[
    \text{tr}(\Delta^T\pi_N(Z)) = \left< \Delta, \pi_N(Z) \right>  = 0. 
\]
Therefore, we see that 
\[
    \{\pi_N(Z) \mid \text{ for all } Z \text{ is a } n\times p \text{ matrix}\} \subset \text{normal space at } Y
\]
But since they have the same dimension, we must conclude that 
\[
    \{\pi_N(Z) \mid \text{ for all } Z \text{ is a } n\times p \text{ matrix}\} = \text{ normal space }.
\]
This is because $\text{dim} T_Y V_{p,n} = np - \frac{p(p+1)}{2} = \frac{p(p+1)}{2} + p(n-p)$.

Note 
\begin{align*}
    \pi_N(Z) = Y\text{skew}(Y^T) + (I - YY^T)Z
\end{align*}
is perpendicular to $\pi_N(Z)$ since 
\[
    \left< \pi_N(Z), \pi_T(Z) \right> = 0.
\]
which implies that 
\begin{align*}
    \text{tr}([\pi_n(Z)]^T\pi_T(Z)) = \text{tr}[\text{sym}(Y^TZ)]^TY^T[Y\text{skew}(Y^TZ) + (I-YY^T)Z]\\
    = \text{tr}[\text{sym}(Y^TZ)(0) + 0]\\
    = 0.
\end{align*}
In a similar fashion, this  implies  that $\text{dim}\pi_T(Z) = \text{dim}T_YV_{p,n}$.


\end{document}